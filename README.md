# CombPlex

**CombPlex** (COMBinatorial multiPLEXing) is a combinatorial staining platform coupled with an algorithmic framework to exponentially increase the number of proteins that can be measured using any imaging modality. In CombPlex, every protein is imaged in several channels, and every channel contains agglomerated images of several proteins. These combinatorically-compressed images are then decompressed to individual protein-images using deep learning and optimization.

## CombPlex Demonstration Notebook
Explore the `CombPlex_demo.ipynb` Jupyter Notebook for a guided walkthrough that illustrates our method, **CombPlex**. We suggest downloading our Git directory, placing it within your Google Drive folder, and then opening `CombPlex_demo.ipynb` using Google Colab.

## System requirements
The code is compatible with Python 3.9 and PyTorch 2.0, and GPU access is required.
You can create an anaconda environment called **CombPlex** with the required dependencies by running:
```
conda env create -f environment.yml
conda activate CombPlex
```

## Data Preparation
To get started, ensure that your training, validation, and test datasets are placed within the `datasets` folder.

Within each dataset folder, you should create subfolders for each FOV. Inside these FOV subfolders, you can organize the images related to that particular FOV, either single-protein images or compressed images. It's essential to maintain identical filenames across all FOV folders and ensure that all image files are in the TIF format.
```
<dataset_name>/
├── <FOV_name_1>/
│   ├── <image_name_1.tif>
│   ├── <image_name_2.tif>
│   │   ├── ...
├── <FOV_name_2>/
│   ├── <image_name_1.tif>
│   ├── <image_name_2.tif>
│   │   ├── ...
├── ...
```
**Note:** You might find the demo datasets helpful for understanding the expected data structure.

## Generating Compression Matrices CSV File
To create the required compression matrices, begin by utilizing the `compression_matrix_builder.py` file. Within this script, complete the specified parameters and execute it. The result will be an CSV file situated within the compression_matrices f`older. Notice you need to input filenames manually. Additionally, you have the option to modify the training or test matrix based on your specific requirements.

If you intend to employ CODEX and perform inference on experimental compressed images, you should also execute the `compression_matrix_adjuster.py` file. 

**Note:** For guidance and reference, you can explore the compression matrix files we've already generated for the experiments conducted in our paper

## Model Training Procedure
1. Follow the data preparation instructions outlined earlier.
2. Create your compression matrices CSV file as explained.
3. Customize the `config/config_train.yaml` according to your preferences.
4. Initiate model training by executing the `train.py` file.
5. If you wish to conduct an ensemble run, you can potentially iterate through steps 3 and 4 for multiple runs.

### Output files:
Upon completion of the training process, your trained models will be located within the `pretrained_models/{model_name}` directory. We suggest selecting the most recent optimally trained model. The term "optimal" denotes a model that achieved the lowest loss in the validation score thus far. However, you have the option to review performances and losses via W&B (Weights & Biases) and make an alternative decision if needed.

Once you've chosen your preferred trained model, kindly duplicate it into the `pretrained_models` folder and rename it to: `{model_name}_optimal.pt`.

## Model Inference Procedure
1. Follow the data preparation instructions outlined earlier.
3. Customize the `config/config_inference.yaml` according to your preferences.
4. Execute the `inference.py` file to run the model's inference on the designated test set.

### Output files:
Upon completing the inference process, the `results/results_folder_name` directory will contain the following files:

1. Compressed images: `{compressed_image_name}.tif`
2. Predicted binary masks for the single-protein images, generated by the trained model: `pred_{protein_name}.tif`
3. Final reconstructed single-protein images: `recon_{protein_name}_{f1_score}.tif`
4. Ground truth single-protein images (if provided): `{protein_name}.tif`
5. A CSV file containing a table of F1 scores: `F1 scores results - {results_folder_name}.csv`.

## Analyze results

Inside the `evaluations` folder, there afe two distinct types of analyses, which are in line with the explanations provided in our paper:

1. Box plot illustrating F1 scores per protein across all Field of Views (FOVs) within the test set: `evaluations/F1_box_plot.py`
2. Box plot representing Pearson correlation coefficients between prediction and ground truth intensities: `evaluations/pearson_corrs.py`

**Note:** Evaluation processes are feasible solely when actual ground truth single-protein images are available.
